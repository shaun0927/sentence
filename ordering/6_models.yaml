# ordering/6_models.yaml
#
# 여러 모델을 번갈아 채점할 수 있게 ‘리스트’ 형태로 작성합니다.
# ─────────────────────────────────────────────────────────────
# key            : 임의의 식별자 (알파벳·숫자·언더스코어)
#   repo         : HF 모델 ID
#   dtype        : bf16 | fp16 | int8 | gptq | awq …
#   tp           : 텐서-병렬 수 (vLLM --tp)
#   max_seq_len  : 채점용 최대 컨텍스트 길이
#   batch        : PPL 평가 시 한 번에 넣을 문장 수
#   remarks      : (선택) 비고
# ─────────────────────────────────────────────────────────────

models:
  - name: motif_102b_bf16                # 아무 식별자
    server_url: http://localhost:8000
    hf_id: moreh/Llama-3-Motif-102B
    batch_size: 1
    gpu_memory_utilization: 0.9
    n_sample: 32                        # ★ self-consistency 샘플 수
    temperature: 0.7                    # ★ 0.6~0.8 권장
    top_p: 0.9               
    dtype: bfloat16                # 선택: 서버가 다른 dtype이면 맞춰 적기

# 예비로 다른 모델들을 추가하고 싶다면 아래처럼 이어서 작성
#klue_roberta_large:
#  repo: klue/roberta-large
#  dtype: fp16
#  tp: 1
#  max_seq_len: 4096
#  batch: 64
