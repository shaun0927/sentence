#!/usr/bin/env python
"""
2_train_fs.py
-------------
KoBERT ì²« ë¬¸ì¥(binary) ë¶„ë¥˜ê¸° íŒŒì¸íŠœë‹
  â€¢ ë‹¨ì¼ í•™ìŠµ (--train / --valid)
  â€¢ K-Fold CV  (--cv  --fold_dir data/fold)  â† fold_0.jsonl â€¦ fold_N.jsonl
     â†³ ê²€ì¦ ë¬¸ì„œê°€ í•™ìŠµ ì„¸íŠ¸ì— ì„ì´ì§€ ì•Šë„ë¡ ëˆ„ìˆ˜ ì°¨ë‹¨
"""
import argparse, yaml, pathlib, random, os, json, inspect
from typing import Optional, Set, List

import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding, IntervalStrategy
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEED = 42
os.environ["TOKENIZERS_PARALLELISM"] = "false"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed: int = SEED):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ metric í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_metrics(pred, k=2):
    y_true = pred.label_ids
    logits = pred.predictions          # [N, 2]

    # â”€â”€ Top-1 F1 ê¸°ì¡´ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    y_pred = logits.argmax(-1)
    p, r, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="binary", zero_division=0
    )

    # â”€â”€ Hit@2 ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í™•ë¥ ì´ ì–‘ì„±ì¼ top-2 ë¬¸ì¥ ê¸°ì¤€ì´ ì•„ë‹ˆë¼,
    # "ì²« ë¬¸ì¥ì¼ í™•ë¥ " ê°’ ìƒìœ„ k ì— ì •ë‹µ(ë¼ë²¨=1)ì´ ìˆë‚˜?
    probs_first = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()

    # ê·¸ë£¹(ë¬¸ì„œ)ë³„ë¡œ top-k ë³´ê¸° â†’ ì—¬ê¸°ì„  ë°°ì¹˜ê°€ ë¬¸ì„œ ì„ì—¬ ìˆìœ¼ë‹ˆ
    # valid ë°ì´í„°ì…‹ì´ ë¬¸ì„œ ë‹¨ìœ„ 4ë¬¸ì¥ì”© ìˆœì„œëŒ€ë¡œë¼ëŠ” ì „ì œë¥¼ ì‚¬ìš©
    hit2, mrr2 = 0, 0.0
    for i in range(0, len(y_true), 4):
        idx = slice(i, i+4)
        topk = probs_first[idx].argsort()[::-1][:k]
        labels4 = y_true[idx]
        # ì •ë‹µ ìœ„ì¹˜
        rank = np.where(labels4[topk] == 1)[0]
        if rank.size:                 # ì„±ê³µ
            hit2 += 1
            mrr2 += 1.0 / (rank[0] + 1)   # rank[0]=0 â†’ 1.0
    n_docs = len(y_true) // 4

    return {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": p, "recall": r, "f1": f1,
        "hit@2": hit2 / n_docs,
        "mrr@2": mrr2 / n_docs,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ strategy í‚¤ ìë™ ê°ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pick_param(long: str, short: str) -> str:
    params = inspect.signature(TrainingArguments.__init__).parameters
    return long if long in params else short


STRAT_EVAL = pick_param("evaluation_strategy", "eval_strategy")
STRAT_SAVE = pick_param("save_strategy",        "save_steps")
STRAT_LOG  = pick_param("logging_strategy",     "logging_steps")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Trainer í™•ì¥ (ê°€ì¤‘ì¹˜ CE) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class WeightedCETrainer(Trainer):
    def __init__(self, class_weight, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._loss_fct = torch.nn.CrossEntropyLoss(weight=class_weight)

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        loss = self._loss_fct(outputs.logits, labels)
        return (loss, outputs) if return_outputs else loss


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•™ìŠµ 1íšŒ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_once(cfg: dict,
               train_path: str,
               valid_path: str,
               exp_path: pathlib.Path,
               exclude_texts: Optional[Set[str]] = None):
    """
    exclude_texts : í•™ìŠµ ì„¸íŠ¸ì—ì„œ ì œê±°í•  ë¬¸ì¥ set (ëˆ„ìˆ˜ ì°¨ë‹¨ìš©)
    """
    set_seed()
    tok = AutoTokenizer.from_pretrained(cfg["model_name"], use_fast=False)
    model = AutoModelForSequenceClassification.from_pretrained(
        cfg["model_name"], num_labels=2
    ).to(device)

    # â”€â”€ ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def encode(ex):
        return tok(ex["text"],
                   truncation=True,
                   max_length=cfg.get("max_len", 96),
                   return_token_type_ids=False)

    raw_train = load_dataset("json", data_files=train_path)["train"]
    if exclude_texts:
        raw_train = raw_train.filter(lambda ex: ex["text"] not in exclude_texts)

    ds_train = raw_train.shuffle(seed=SEED).map(encode, batched=True)
    ds_valid = load_dataset("json", data_files=valid_path)["train"] \
               .map(encode, batched=True)

    # â”€â”€ í•™ìŠµ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    exp_path.mkdir(parents=True, exist_ok=True)
    train_args = TrainingArguments(
        output_dir=str(exp_path),
        per_device_train_batch_size=cfg["batch_size"],
        per_device_eval_batch_size=cfg["batch_size"],
        learning_rate=float(cfg["lr"]),
        num_train_epochs=cfg["epochs"],
        fp16=torch.cuda.is_available(),
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        logging_steps=50,
        seed=SEED,
        **{
            STRAT_EVAL: IntervalStrategy.EPOCH,
            STRAT_SAVE: IntervalStrategy.EPOCH,
            STRAT_LOG : IntervalStrategy.EPOCH,
        }
    )

    class_w = torch.tensor([1.0, cfg.get("pos_weight", 3.0)], device=device)
    trainer = WeightedCETrainer(
        class_weight=class_w,
        model=model,
        args=train_args,
        train_dataset=ds_train,
        eval_dataset=ds_valid,
        tokenizer=tok,
        data_collator=DataCollatorWithPadding(tok),
        compute_metrics=compute_metrics,
    )

    result = trainer.train()
    best_dir = exp_path / "best"
    trainer.save_model(str(best_dir)); tok.save_pretrained(str(best_dir))

    metrics = trainer.evaluate()
    metrics.update({"train_runtime": result.metrics["train_runtime"]})
    return metrics, best_dir


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(args):
    cfg = yaml.safe_load(open(args.cfg, encoding="utf-8"))
    if args.epochs:
        cfg["epochs"] = args.epochs

    ckpt_root = pathlib.Path("first_sentence_kobert/checkpoints")

    # â”€â”€ ë‹¨ì¼ í•™ìŠµ ëª¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not args.cv:
        exp_path = ckpt_root / args.exp
        metrics, best_dir = train_once(cfg, args.train, args.valid, exp_path)
        print("âœ… Single run finished:", json.dumps(metrics, indent=2, ensure_ascii=False))
        print("Best model saved ->", best_dir)
        return

    # â”€â”€ K-Fold CV ëª¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    fold_dir = pathlib.Path(args.fold_dir)
    fold_files: List[pathlib.Path] = sorted(fold_dir.glob("fold_*.jsonl"))
    assert fold_files, f"{fold_dir} ì— fold_*.jsonl ì´ ì—†ìŠµë‹ˆë‹¤."
    K = len(fold_files)
    print(f"â–¶ Detected {K} fold files")

    results = []
    for k, valid_file in enumerate(fold_files):
        # â”€â”€ ëˆ„ìˆ˜ ì°¨ë‹¨ìš© exclude set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        valid_ds = load_dataset("json", data_files=str(valid_file))["train"]
        exclude_set = set(valid_ds["text"])

        exp_path = ckpt_root / f"{args.exp}_fold{k}"
        print(f"\nâ”€â”€â”€ Fold {k} training â”€â”€â”€")
        m, _ = train_once(cfg,
                          train_path=args.train,
                          valid_path=str(valid_file),
                          exp_path=exp_path,
                          exclude_texts=exclude_set)
        m["fold"] = k
        results.append(m)
        print(f"Fold {k} metrics:", m)

    f1s = [m["eval_f1"] for m in results]
    print(f"\nğŸ“Š {K}-Fold CV summary:  avg F1 = {np.mean(f1s):.4f}  Â±  {np.std(f1s):.4f}")

    with open(ckpt_root / f"{args.exp}_cv_metrics.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print("Metrics logged to", f.name)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--train", required=True, help="fs_train.jsonl (ì „ì²´)")
    p.add_argument("--valid", help="single-run valid jsonl")
    p.add_argument("--cfg",   required=True, help="yaml config")
    p.add_argument("--exp",   required=True, help="experiment name")
    p.add_argument("--epochs", type=int, help="override epochs")

    # CV ì˜µì…˜
    p.add_argument("--cv", action="store_true",
                   help="enable K-Fold cross-validation (fold_*.jsonl ìë™ íƒìƒ‰)")
    p.add_argument("--fold_dir", default="data/fold",
                   help="directory containing fold_0.jsonl â€¦ fold_N.jsonl")
    args = p.parse_args()

    if args.cv:
        assert pathlib.Path(args.fold_dir).is_dir(), "--fold_dir ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    else:
        if not args.valid:
            raise ValueError("single-run ëª¨ë“œì—ì„œëŠ” --valid ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    main(args)
